"""
ì»¬ëŸ¼ ìë™ ì¶”ë¡  ëª¨ë“ˆ
CSV/Excel ì»¬ëŸ¼ì„ Process Mining í•„ìˆ˜ í•„ë“œ(Case ID / Activity / Timestamp)ì—
í‚¤ì›Œë“œ + íƒ€ì… + í†µê³„ ê¸°ë°˜ìœ¼ë¡œ ìë™ ë§¤í•‘í•©ë‹ˆë‹¤.
"""
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Optional

import pandas as pd

# â”€â”€â”€ í‚¤ì›Œë“œ ì‚¬ì „ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KEYWORDS: dict[str, dict[str, set]] = {
    "case_id": {
        "exact": {
            "caseid", "case_id", "traceid", "trace_id", "instanceid",
            "process_id", "orderid", "order_id",
        },
        "partial": {
            "case", "trace", "instance", "order", "id", "key",
            "no", "num", "number", "code",
            "ë²ˆí˜¸", "ì½”ë“œ", "ì£¼ë¬¸", "ì¼€ì´ìŠ¤", "ê±´", "ì‹ë³„",
        },
    },
    "activity": {
        "exact": {
            "activity", "activityname", "activity_name", "task",
            "event", "action", "step", "concept:name", "eventname",
            "taskname", "conceptname",
        },
        "partial": {
            "act", "task", "event", "action", "step", "name", "type",
            "í™œë™", "ì‘ì—…", "ì´ë²¤íŠ¸", "ì—…ë¬´", "ë‹¨ê³„", "í™œë™ëª…", "ì‘ì—…ëª…",
        },
    },
    "timestamp": {
        "exact": {
            "timestamp", "time", "datetime", "date", "time:timestamp",
            "starttime", "start_time", "endtime", "end_time",
            "completetime", "complete_time", "createdat", "eventtime",
        },
        "partial": {
            "time", "date", "dt", "ts", "at", "when",
            "start", "end", "complete", "created",
            "ì‹œê°", "ì‹œê°„", "ì¼ì‹œ", "ë‚ ì§œ", "ì¼ì", "íƒ€ì„",
        },
    },
    "resource": {
        "exact": {
            "resource", "org:resource", "user", "performer",
            "assignee", "operator", "employee",
        },
        "partial": {
            "resource", "user", "person", "agent", "role",
            "ë‹´ë‹¹ì", "ì‚¬ìš©ì", "ìˆ˜í–‰ì", "ì‘ì—…ì", "ë‹´ë‹¹", "ì§ì›",
        },
    },
}

_CONFIDENCE_TABLE = [
    (80, "high",   "ğŸŸ¢ ë†’ìŒ"),
    (50, "medium", "ğŸŸ¡ ë³´í†µ"),
    (30, "low",    "ğŸŸ  ë‚®ìŒ"),
    ( 0, "failed", "ğŸ”´ ì‹¤íŒ¨"),
]


# â”€â”€â”€ ë°ì´í„° í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class ColumnProfile:
    name: str
    dtype: str
    null_ratio: float
    unique_count: int
    unique_ratio: float
    sample_values: list
    is_parseable_datetime: bool
    avg_str_length: float = 0.0


@dataclass
class MappingResult:
    field: str                                    # "case_id" | "activity" | ...
    column: Optional[str]                         # ë§¤í•‘ëœ ì»¬ëŸ¼ëª… (ì—†ìœ¼ë©´ None)
    score: float                                  # 0~100
    confidence_level: str                         # "high" | "medium" | "low" | "failed"
    confidence_label: str                         # "ğŸŸ¢ ë†’ìŒ" ë“±
    alternatives: list[tuple[str, float]] = field(default_factory=list)


# â”€â”€â”€ ë‚´ë¶€ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _try_parse_datetime(series: pd.Series, sample_size: int = 50) -> float:
    """datetime íŒŒì‹± ì„±ê³µë¥ ì„ ë°˜í™˜ (0~1)."""
    sample = series.dropna().head(sample_size)
    if len(sample) == 0:
        return 0.0
    try:
        parsed = pd.to_datetime(sample, errors="coerce")
        return float(parsed.notna().sum()) / len(sample)
    except Exception:
        return 0.0


def _profile_columns(df: pd.DataFrame) -> list[ColumnProfile]:
    """ëª¨ë“  ì»¬ëŸ¼ì˜ í”„ë¡œí•„ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    total = max(len(df), 1)
    profiles = []

    for col in df.columns:
        series = df[col]
        null_ratio = float(series.isna().sum()) / total
        unique_count = int(series.nunique())
        unique_ratio = unique_count / total
        sample_vals = series.dropna().head(50).tolist()
        is_dt = _try_parse_datetime(series) >= 0.85

        avg_len = 0.0
        if series.dtype == object:
            lengths = series.dropna().astype(str).str.len()
            avg_len = float(lengths.mean()) if len(lengths) > 0 else 0.0

        profiles.append(ColumnProfile(
            name=col,
            dtype=str(series.dtype),
            null_ratio=null_ratio,
            unique_count=unique_count,
            unique_ratio=unique_ratio,
            sample_values=sample_vals,
            is_parseable_datetime=is_dt,
            avg_str_length=avg_len,
        ))
    return profiles


def _keyword_score(col_name: str, role: str) -> float:
    """í‚¤ì›Œë“œ ë§¤ì¹­ ìŠ¤ì½”ì–´ (0~80)."""
    normalized = col_name.lower().replace(" ", "").replace("_", "").replace(":", "")
    kw = KEYWORDS.get(role, {})

    exact_set = {k.replace("_", "").replace(":", "") for k in kw.get("exact", set())}
    if normalized in exact_set:
        return 80.0

    for kw_part in kw.get("partial", set()):
        if kw_part.lower() in normalized:
            return 40.0

    return 0.0


def _type_score(profile: ColumnProfile, role: str) -> float:
    """ë°ì´í„° íƒ€ì… ê¸°ë°˜ ìŠ¤ì½”ì–´."""
    dtype = profile.dtype

    if role == "timestamp":
        if "datetime" in dtype:
            return 60.0
        if profile.is_parseable_datetime:
            return 50.0
        if ("int" in dtype or "float" in dtype) and profile.sample_values:
            if any(1e9 < v < 2e9 for v in profile.sample_values
                   if isinstance(v, (int, float))):
                return 30.0  # UNIX timestamp ê°€ëŠ¥ì„±
        return -20.0 if ("int" in dtype or "float" in dtype) else 0.0

    elif role == "case_id":
        if "int" in dtype:
            return 25.0
        if dtype == "object":
            return 20.0
        if "float" in dtype:
            return -10.0

    elif role == "activity":
        if dtype == "object":
            return 30.0
        if "int" in dtype or "float" in dtype:
            return -20.0

    elif role == "resource":
        if dtype == "object":
            return 25.0

    return 0.0


def _stats_score(profile: ColumnProfile, role: str) -> float:
    """í†µê³„ì  íŠ¹ì„± ê¸°ë°˜ ìŠ¤ì½”ì–´."""
    ur = profile.unique_ratio
    nr = profile.null_ratio
    score = 0.0

    if role == "case_id":
        # ì´ë²¤íŠ¸ ë¡œê·¸ì—ì„œ case_idì˜ unique_ratioëŠ” ì´ë²¤íŠ¸ ìˆ˜ / ì¼€ì´ìŠ¤ ìˆ˜ì— ë”°ë¼ ë‹¤ì–‘
        # ì˜ˆ: ì¼€ì´ìŠ¤ë‹¹ 7ì´ë²¤íŠ¸ â†’ urâ‰ˆ0.14, ì¼€ì´ìŠ¤ë‹¹ 2ì´ë²¤íŠ¸ â†’ urâ‰ˆ0.5
        if ur >= 0.5:
            score += 40.0
        elif ur >= 0.05:   # ì¼ë°˜ ì´ë²¤íŠ¸ ë¡œê·¸ì˜ ì „í˜•ì  ë²”ìœ„
            score += 28.0
        elif ur >= 0.01:
            score += 8.0
        else:
            score -= 20.0
        score += 10.0 if nr == 0.0 else (5.0 if nr <= 0.05 else -20.0)

    elif role == "activity":
        if 0.001 <= ur <= 0.1:
            score += 40.0
        elif ur <= 0.3:
            score += 20.0
        elif ur > 0.3:
            score -= 10.0
        if 2.0 <= profile.avg_str_length <= 30.0:
            score += 15.0

    elif role == "timestamp":
        if profile.is_parseable_datetime:
            score += 50.0
        score += 10.0 if nr <= 0.05 else -10.0

    elif role == "resource":
        if 0.001 <= ur <= 0.2:
            score += 30.0
        elif ur <= 0.5:
            score += 10.0
        else:
            score -= 10.0

    return score


def _score_column(profile: ColumnProfile, role: str) -> float:
    """ì»¬ëŸ¼ì˜ íŠ¹ì • ì—­í•  ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (0~100)."""
    kw = _keyword_score(profile.name, role)
    ty = _type_score(profile, role)
    st = _stats_score(profile, role)
    return max(0.0, min(100.0, kw * 0.35 + ty * 0.35 + st * 0.30))


def _get_confidence(score: float) -> tuple[str, str]:
    for threshold, level, label in _CONFIDENCE_TABLE:
        if score >= threshold:
            return level, label
    return "failed", "ğŸ”´ ì‹¤íŒ¨"


# â”€â”€â”€ ë©”ì¸ í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ColumnMapper:
    """DataFrame ì»¬ëŸ¼ì„ Process Mining í•„ë“œì— ìë™ ë§¤í•‘í•©ë‹ˆë‹¤."""

    REQUIRED_FIELDS = ["case_id", "activity", "timestamp"]
    OPTIONAL_FIELDS = ["resource"]
    ALL_FIELDS = REQUIRED_FIELDS + OPTIONAL_FIELDS

    def map(self, df: pd.DataFrame) -> list[MappingResult]:
        """ì»¬ëŸ¼ ë§¤í•‘ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        profiles = _profile_columns(df)

        score_matrix = {
            f: {p.name: _score_column(p, f) for p in profiles}
            for f in self.ALL_FIELDS
        }

        assigned = self._resolve_conflicts(score_matrix)

        results = []
        for f in self.ALL_FIELDS:
            col = assigned.get(f)
            score = score_matrix[f].get(col, 0.0) if col else 0.0
            level, label = _get_confidence(score)
            alts = sorted(
                [(c, s) for c, s in score_matrix[f].items() if c != col],
                key=lambda x: x[1],
                reverse=True,
            )[:3]
            results.append(MappingResult(
                field=f,
                column=col,
                score=score,
                confidence_level=level,
                confidence_label=label,
                alternatives=alts,
            ))
        return results

    def _resolve_conflicts(
        self, score_matrix: dict[str, dict[str, float]]
    ) -> dict[str, Optional[str]]:
        """ì¤‘ë³µ ì—†ì´ ê° í•„ë“œì— ì»¬ëŸ¼ì„ í• ë‹¹í•©ë‹ˆë‹¤."""
        assigned: dict[str, Optional[str]] = {}
        used: set[str] = set()

        # ê°€ì¥ ìì‹  ìˆëŠ” í•„ë“œë¶€í„° ì²˜ë¦¬ (greedy)
        fields_sorted = sorted(
            self.ALL_FIELDS,
            key=lambda f: max(score_matrix[f].values()) if score_matrix[f] else 0,
            reverse=True,
        )

        for f in fields_sorted:
            candidates = sorted(
                [(c, s) for c, s in score_matrix[f].items() if c not in used],
                key=lambda x: x[1],
                reverse=True,
            )
            if candidates and candidates[0][1] >= 20.0:
                best = candidates[0][0]
                assigned[f] = best
                used.add(best)
            else:
                assigned[f] = None

        return assigned

    def validate(
        self, df: pd.DataFrame, mapping: dict[str, Optional[str]]
    ) -> list[dict]:
        """
        ë§¤í•‘ ê²°ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.

        Returns
        -------
        list of {"level": "error" | "warning", "message": str}
        """
        msgs = []
        total = max(len(df), 1)

        for f in self.REQUIRED_FIELDS:
            col = mapping.get(f)
            if not col or col not in df.columns:
                msgs.append({"level": "error",
                             "message": f"í•„ìˆ˜ í•„ë“œ '{f}'ê°€ ë§¤í•‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."})
                continue

            series = df[col]
            null_ratio = series.isna().sum() / total

            if f == "case_id" and null_ratio > 0:
                msgs.append({"level": "error",
                             "message": f"Case ID ì»¬ëŸ¼ '{col}'ì— ê²°ì¸¡ê°’ì´ ìˆìŠµë‹ˆë‹¤."})

            elif f == "activity":
                if null_ratio > 0:
                    msgs.append({"level": "error",
                                 "message": f"Activity ì»¬ëŸ¼ '{col}'ì— ê²°ì¸¡ê°’ì´ ìˆìŠµë‹ˆë‹¤."})
                if series.nunique() < 2:
                    msgs.append({"level": "warning",
                                 "message": "Activity ì¢…ë¥˜ê°€ 1ê°œë¿ì…ë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."})

            elif f == "timestamp":
                if null_ratio > 0.05:
                    msgs.append({"level": "warning",
                                 "message": f"Timestamp ì»¬ëŸ¼ '{col}'ì— ê²°ì¸¡ê°’ì´ {null_ratio*100:.1f}% ìˆìŠµë‹ˆë‹¤."})
                try:
                    pd.to_datetime(series.dropna().head(10), errors="raise")
                except Exception:
                    msgs.append({"level": "error",
                                 "message": f"Timestamp ì»¬ëŸ¼ '{col}'ì˜ ë‚ ì§œ í˜•ì‹ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."})

        case_col = mapping.get("case_id")
        if case_col and case_col in df.columns and df[case_col].nunique() < 2:
            msgs.append({"level": "error",
                         "message": "ì¼€ì´ìŠ¤ ìˆ˜ê°€ 1ê°œì…ë‹ˆë‹¤. ìµœì†Œ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤."})

        if total < 10:
            msgs.append({"level": "warning",
                         "message": f"ì´ë²¤íŠ¸ ìˆ˜ê°€ {total}ê°œë¡œ ë§¤ìš° ì ìŠµë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."})

        return msgs
